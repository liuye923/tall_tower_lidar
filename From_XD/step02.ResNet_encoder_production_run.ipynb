{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e87c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import datetime as dt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import rescale as skrescale\n",
    "from scipy import signal as ssignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a94b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vstring = int(sys.argv[1])\n",
    "rseed = int(42)\n",
    "#humidvar = sys.argv[3]\n",
    "import random\n",
    "random.seed(rseed)\n",
    "np.random.seed(rseed)\n",
    "torch.manual_seed(rseed)\n",
    "torch.cuda.manual_seed(rseed)\n",
    "torch.cuda.manual_seed_all(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0553de",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model = 'R18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7890a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24e87556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCL(nn.Module):\n",
    "    \"\"\"\n",
    "    We opt for simplicity and adopt the commonly used ResNet (He et al., 2016) to obtain hi = f(x ̃i) = ResNet(x ̃i) where hi ∈ Rd is the output after the average pooling layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train, encoder, projection_dim, n_features):\n",
    "        super(SCL, self).__init__()\n",
    "        self.train = train\n",
    "        self.encoder = encoder\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # increse input channel to 6\n",
    "        layer = self.encoder.conv1\n",
    "        new_nc = 6\n",
    "        new_layer = nn.Conv2d(in_channels=new_nc,\n",
    "                              out_channels=layer.out_channels,\n",
    "                              kernel_size=layer.kernel_size,\n",
    "                              stride=layer.stride,\n",
    "                              padding=layer.padding,\n",
    "                              bias=layer.bias)\n",
    "        # Extending the weights by copying from the old 3 to the new 3 channels\n",
    "        new_layer.weight.data[:, 0:3, :, :] = layer.weight.clone()\n",
    "        new_layer.weight.data[:, 3:6, :, :] = layer.weight.clone()\n",
    "        new_layer.weight = nn.Parameter(new_layer.weight)\n",
    "        self.encoder.conv1 = new_layer\n",
    "\n",
    "        # Replace the fc layer with an Identity function\n",
    "        self.encoder.fc = Identity()\n",
    "        # We use a MLP with one hidden layer to obtain z_i = g(h_i) = W(2)σ(W(1)h_i) where σ is a ReLU non-linearity.\n",
    "        # xc: This is the part that needs to be trained\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.n_features, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_features, projection_dim, bias=False),\n",
    "        )\n",
    "        # These are the parameters obtained from simCLR repo. I have also patched it to include 6 channels at the conv1\n",
    "        param_file = rootdir + 'model_lib/SCL_param.encoder.%s.6_channel.init.tar' % opt_model\n",
    "        self.encoder.load_state_dict(torch.load(param_file, map_location='cpu'))\n",
    "\n",
    "\n",
    "        # freeze the encoder so it is not re-trained\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x_i):\n",
    "        # z_i = self.encoder(x_i.type(torch.FloatTensor).cuda()).type(torch.FloatTensor).cuda()\n",
    "        z_i = self.encoder(x_i.type(torch.FloatTensor)).type(torch.FloatTensor)\n",
    "\n",
    "        del x_i\n",
    "        return z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2286bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_norm_data_by_var(my_var):\n",
    "\n",
    "    sfile_max = datadir + 'agg_40year/minmax/pt.max.%s.nc' % my_var\n",
    "    sfile_min = datadir + 'agg_40year/minmax/pt.min.%s.nc' % my_var\n",
    "\n",
    "    with xr.open_dataset(sfile_max) as inds:\n",
    "        ds_Vmax = inds[my_var].values\n",
    "\n",
    "    with xr.open_dataset(sfile_min) as inds:\n",
    "        ds_Vmin = inds[my_var].values\n",
    "        \n",
    "    print(ds_Vmax, ds_Vmin)\n",
    "\n",
    "    my_vmax = np.maximum(ds_Vmax, -1*ds_Vmin)\n",
    "    Vmax, Vmin = my_vmax, -1*my_vmax\n",
    "    \n",
    "    infile = datadir + 'ERA5.SCL.850mb_anomaly.%s.1981-2020.daymean.nc' % (my_var)\n",
    "    print('loading data from %s ...' % infile)\n",
    "    with xr.open_dataset(infile) as inds:\n",
    "        outdata = (inds[my_var].values[:,0,:,:]-Vmin)/(Vmax-Vmin)\n",
    "\n",
    "    return outdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695ffd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean5kernel = np.ones((5,5))/25\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    '''\n",
    "    Since we need to mannually normalize the data, let's create datasets elsewhere, and just aggreagate them here.\n",
    "    Requires: T_full, H_full, W_full, U_full, V_full, Z_full\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_data = root_dir + 'ERA5.SCL.850mb_anomaly.W.1981-2020.daymean.nc'\n",
    "\n",
    "    def __len__(self):\n",
    "        with xr.open_dataset(self.sample_data) as inds:\n",
    "            nt = inds['W'].shape[0]\n",
    "        return int(nt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # find a corresponding idx_pair, outside the 360-length window of idx\n",
    "        # idx_pair = xxxx\n",
    "\n",
    "\n",
    "        sample_raw = np.zeros((6,102,202))\n",
    "\n",
    "        for i,fullds in zip(np.arange(6), [H_full, T_full, W_full, U_full, V_full, Z_full]):\n",
    "            \n",
    "            # construct input for idx\n",
    "            # rescaling\n",
    "            data_step1 = skrescale(fullds[idx], (2.5, 2.5), anti_aliasing=True)\n",
    "            # mean using 5x5\n",
    "            data_step2 = ssignal.convolve2d(data_step1, mean5kernel, boundary='symm', mode='same')\n",
    "            sample_raw[i] = data_step2\n",
    "\n",
    "\n",
    "            # construct input for idx_pair\n",
    "            # rescaling\n",
    "\n",
    "            # mean using 5x5\n",
    "\n",
    "        return sample_raw, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a9ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCLloss(my_x, my_y, my_temperature=0.5):\n",
    "    '''\n",
    "    my_x and my_y has a one-to-one pair. So there are in total N*N pairs. In these N*N, the diagonal pairs are positive,\n",
    "     and the rest are negative. So we want to maximum diagonal while suppressing the rest.\n",
    "    '''\n",
    "    ns = my_x.shape[0]\n",
    "    # use broadcasting to achieve pairwise cos. Note my_y.t() operation and dimension handling\n",
    "    cos_matrix = torch.nn.functional.cosine_similarity(my_x[:,:,None], my_y.t()[None,:,:])/my_temperature\n",
    "    similarity_matrix = torch.exp(cos_matrix)\n",
    "\n",
    "\n",
    "    loss = torch.tensor([0.0], requires_grad=True)\n",
    "    for i in np.arange(ns):\n",
    "        loss = loss -1*torch.log(similarity_matrix[i,i]/(torch.sum(similarity_matrix[i,:])-similarity_matrix[i,i]))\n",
    "        loss = loss -1*torch.log(similarity_matrix[i,i]/(torch.sum(similarity_matrix[:,i])-similarity_matrix[i,i]))\n",
    "\n",
    "    loss = loss/(2*ns)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2f3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/'\n",
    "\n",
    "datadir = '/global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357b07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. major parameters\n",
    "if opt_model=='R18':\n",
    "    batch_size = 128\n",
    "elif opt_model=='R15':\n",
    "    batch_size = 64\n",
    "    \n",
    "# 1. construct functions\n",
    "if opt_model=='R18':\n",
    "    encoder = torchvision.models.resnet18(weights=None)\n",
    "elif opt_model=='R50':\n",
    "    encoder = torchvision.models.resnet50(weights=None)\n",
    "\n",
    "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
    "\n",
    "# 2. construct two models, one with random parameters, one with pre-trained parameters\n",
    "projection_dim = 256\n",
    "SCL = SCL(True, encoder, projection_dim, n_features)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SCL = SCL.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b56f49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.06845 -76.24047\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.RH.1981-2020.daymean.nc ...\n",
      "20.074585 -34.17395\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.T.1981-2020.daymean.nc ...\n",
      "26.455658 -35.592907\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.U.1981-2020.daymean.nc ...\n",
      "44.648796 -33.113792\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.V.1981-2020.daymean.nc ...\n",
      "7.01952 -5.8575945\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.W.1981-2020.daymean.nc ...\n",
      "3010.4414 -4328.3145\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.Z.1981-2020.daymean.nc ...\n",
      "(14610, 41, 81)\n"
     ]
    }
   ],
   "source": [
    "# 3. load data\n",
    "\n",
    "H_full = collect_norm_data_by_var('RH')\n",
    "T_full = collect_norm_data_by_var('T')\n",
    "U_full = collect_norm_data_by_var('U')\n",
    "V_full = collect_norm_data_by_var('V')\n",
    "W_full = collect_norm_data_by_var('W')\n",
    "Z_full = collect_norm_data_by_var('Z')\n",
    "\n",
    "print(Z_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2acb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(root_dir=datadir)\n",
    "\n",
    "# turn off shuffle, so data is processed in the time order\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f69af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da0b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:46:31.915574\n",
      "0 128\n",
      "128 256\n",
      "256 384\n",
      "384 512\n",
      "512 640\n",
      "640 768\n",
      "768 896\n",
      "896 1024\n",
      "1024 1152\n",
      "1152 1280\n",
      "1280 1408\n",
      "1408 1536\n",
      "1536 1664\n",
      "1664 1792\n",
      "1792 1920\n",
      "1920 2048\n",
      "2048 2176\n",
      "2176 2304\n",
      "2304 2432\n",
      "2432 2560\n",
      "2560 2688\n",
      "2688 2816\n",
      "2816 2944\n",
      "2944 3072\n",
      "3072 3200\n",
      "3200 3328\n",
      "3328 3456\n",
      "3456 3584\n",
      "3584 3712\n",
      "3712 3840\n",
      "3840 3968\n",
      "3968 4096\n",
      "4096 4224\n",
      "4224 4352\n",
      "4352 4480\n",
      "4480 4608\n",
      "4608 4736\n",
      "4736 4864\n",
      "4864 4992\n",
      "4992 5120\n",
      "5120 5248\n",
      "5248 5376\n",
      "5376 5504\n",
      "5504 5632\n",
      "5632 5760\n",
      "5760 5888\n",
      "5888 6016\n",
      "6016 6144\n",
      "6144 6272\n",
      "6272 6400\n",
      "6400 6528\n",
      "6528 6656\n",
      "6656 6784\n",
      "6784 6912\n",
      "6912 7040\n",
      "7040 7168\n",
      "7168 7296\n",
      "7296 7424\n",
      "7424 7552\n",
      "7552 7680\n",
      "7680 7808\n",
      "7808 7936\n",
      "7936 8064\n",
      "8064 8192\n",
      "8192 8320\n",
      "8320 8448\n",
      "8448 8576\n",
      "8576 8704\n",
      "8704 8832\n",
      "8832 8960\n",
      "8960 9088\n",
      "9088 9216\n",
      "9216 9344\n",
      "9344 9472\n",
      "9472 9600\n",
      "9600 9728\n",
      "9728 9856\n",
      "9856 9984\n",
      "9984 10112\n",
      "10112 10240\n",
      "10240 10368\n",
      "10368 10496\n",
      "10496 10624\n",
      "10624 10752\n",
      "10752 10880\n",
      "10880 11008\n",
      "11008 11136\n",
      "11136 11264\n",
      "11264 11392\n",
      "11392 11520\n",
      "11520 11648\n",
      "11648 11776\n",
      "11776 11904\n",
      "11904 12032\n",
      "12032 12160\n",
      "12160 12288\n",
      "12288 12416\n",
      "12416 12544\n",
      "12544 12672\n",
      "12672 12800\n",
      "12800 12928\n",
      "12928 13056\n",
      "13056 13184\n",
      "13184 13312\n",
      "13312 13440\n",
      "13440 13568\n",
      "13568 13696\n",
      "13696 13824\n",
      "13824 13952\n",
      "13952 14080\n",
      "14080 14208\n",
      "14208 14336\n",
      "14336 14464\n",
      "14464 14592\n",
      "14592 14610\n",
      "2024-06-19 10:51:55.704600\n"
     ]
    }
   ],
   "source": [
    "outdata = np.zeros((H_full.shape[0], n_features))\n",
    "outindex = np.zeros(H_full.shape[0])\n",
    "\n",
    "sindex = -1*batch_size\n",
    "eindex = 0\n",
    "\n",
    "print(dt.datetime.now())\n",
    "for step, data in enumerate(train_dataloader):\n",
    "\n",
    "    testout = SCL(data[0].to(device))\n",
    "\n",
    "\n",
    "    sindex = eindex\n",
    "    eindex += testout.shape[0]\n",
    "\n",
    "    outdata[sindex:eindex, :] = testout.detach().cpu().numpy()\n",
    "    outindex[sindex:eindex] = data[1]\n",
    "\n",
    "    print(sindex, eindex)\n",
    "\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7cc2155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/ResNet_output/RH_input/R18_output.anomaly.daymean.1981-2020.ERA5.mat ...\n"
     ]
    }
   ],
   "source": [
    "outfile = rootdir + 'ResNet_output/RH_input/%s_output.anomaly.daymean.1981-2020.ERA5.mat' % opt_model\n",
    "print('writing to %s ...' % outfile)\n",
    "description = 'Just the simCLR encoder output. So in 512 dimension. Use %s model' % opt_model\n",
    "script = '/global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/step02.ResNet_encoder_production_run.ipynb'\n",
    "sio.savemat(outfile, {'ResNetoutput':outdata, 'tindex':outindex, 'description':description, 'script':script})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3a725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
