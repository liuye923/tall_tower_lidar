{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e87c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import datetime as dt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import rescale as skrescale\n",
    "from scipy import signal as ssignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a94b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vstring = int(sys.argv[1])\n",
    "rseed = int(42)\n",
    "#humidvar = sys.argv[3]\n",
    "import random\n",
    "random.seed(rseed)\n",
    "np.random.seed(rseed)\n",
    "torch.manual_seed(rseed)\n",
    "torch.cuda.manual_seed(rseed)\n",
    "torch.cuda.manual_seed_all(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0553de",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model = 'R18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24e87556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_layers, num_heads, hidden_dim, channels):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder = ViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=hidden_dim,  # Output dimension of the ViT\n",
    "            dim=hidden_dim,\n",
    "            depth=num_layers,\n",
    "            heads=num_heads,\n",
    "            mlp_dim=hidden_dim * 4,\n",
    "            pool='cls',\n",
    "            channels=channels\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x.type(torch.FloatTensor)).type(torch.FloatTensor)\n",
    "        del x\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2286bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_norm_data_by_var(my_var):\n",
    "\n",
    "    sfile_max = datadir + 'agg_40year/minmax/pt.max.%s.nc' % my_var\n",
    "    sfile_min = datadir + 'agg_40year/minmax/pt.min.%s.nc' % my_var\n",
    "\n",
    "    with xr.open_dataset(sfile_max) as inds:\n",
    "        ds_Vmax = inds[my_var].values\n",
    "\n",
    "    with xr.open_dataset(sfile_min) as inds:\n",
    "        ds_Vmin = inds[my_var].values\n",
    "        \n",
    "    print(ds_Vmax, ds_Vmin)\n",
    "\n",
    "    my_vmax = np.maximum(ds_Vmax, -1*ds_Vmin)\n",
    "    Vmax, Vmin = my_vmax, -1*my_vmax\n",
    "    \n",
    "    infile = datadir + 'ERA5.SCL.850mb_anomaly.%s.1981-2020.daymean.nc' % (my_var)\n",
    "    print('loading data from %s ...' % infile)\n",
    "    with xr.open_dataset(infile) as inds:\n",
    "        outdata = (inds[my_var].values[:,0,:,:]-Vmin)/(Vmax-Vmin)\n",
    "\n",
    "    return outdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695ffd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean5kernel = np.ones((5,5))/25\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    '''\n",
    "    Since we need to mannually normalize the data, let's create datasets elsewhere, and just aggreagate them here.\n",
    "    Requires: T_full, H_full, W_full, U_full, V_full, Z_full\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_data = root_dir + 'ERA5.SCL.850mb_anomaly.W.1981-2020.daymean.nc'\n",
    "\n",
    "    def __len__(self):\n",
    "        with xr.open_dataset(self.sample_data) as inds:\n",
    "            nt = inds['W'].shape[0]\n",
    "        return int(nt) - 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # find a corresponding idx_pair, outside the 360-length window of idx\n",
    "        # idx_pair = xxxx\n",
    "\n",
    "        idx_pair = idx + 10\n",
    "\n",
    "\n",
    "        sample_raw = np.zeros((6,102,202))\n",
    "        sample_raw_pair = np.zeros((6,102,202))\n",
    "\n",
    "        for i,fullds in zip(np.arange(6), [H_full, T_full, W_full, U_full, V_full, Z_full]):\n",
    "            \n",
    "            # construct input for idx\n",
    "            # rescaling\n",
    "            data_step1 = skrescale(fullds[idx], (2.5, 2.5), anti_aliasing=True)\n",
    "            # mean using 5x5\n",
    "            data_step2 = ssignal.convolve2d(data_step1, mean5kernel, boundary='symm', mode='same')\n",
    "            sample_raw[i] = data_step2\n",
    "\n",
    "\n",
    "            # construct input for idx_pair\n",
    "            # rescaling\n",
    "            data_step1 = skrescale(fullds[idx_pair], (2.5, 2.5), anti_aliasing=True)\n",
    "            # mean using 5x5\n",
    "            data_step2 = ssignal.convolve2d(data_step1, mean5kernel, boundary='symm', mode='same')\n",
    "            sample_raw_pair[i] = data_step2\n",
    "\n",
    "        return sample_raw, sample_raw_pair, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a9ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCLloss(my_x, my_y, my_temperature=0.5):\n",
    "    '''\n",
    "    my_x and my_y has a one-to-one pair. So there are in total N*N pairs. In these N*N, the diagonal pairs are positive,\n",
    "     and the rest are negative. So we want to maximum diagonal while suppressing the rest.\n",
    "    '''\n",
    "    ns = my_x.shape[0]\n",
    "    # use broadcasting to achieve pairwise cos. Note my_y.t() operation and dimension handling\n",
    "    cos_matrix = torch.nn.functional.cosine_similarity(my_x[:,:,None], my_y.t()[None,:,:])/my_temperature\n",
    "    similarity_matrix = torch.exp(cos_matrix)\n",
    "\n",
    "\n",
    "    loss = torch.tensor([0.0], requires_grad=True)\n",
    "    for i in np.arange(ns):\n",
    "        loss = loss -1*torch.log(similarity_matrix[i,i]/(torch.sum(similarity_matrix[i,:])-similarity_matrix[i,i]))\n",
    "        loss = loss -1*torch.log(similarity_matrix[i,i]/(torch.sum(similarity_matrix[:,i])-similarity_matrix[i,i]))\n",
    "\n",
    "    loss = loss/(2*ns)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2f3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/'\n",
    "\n",
    "datadir = '/global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "357b07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. major parameters\n",
    "if opt_model=='R18':\n",
    "    batch_size = 128\n",
    "elif opt_model=='R15':\n",
    "    batch_size = 64\n",
    "    \n",
    "# 1. construct functions\n",
    "if opt_model=='R18':\n",
    "    encoder = torchvision.models.resnet18(weights=None)\n",
    "elif opt_model=='R50':\n",
    "    encoder = torchvision.models.resnet50(weights=None)\n",
    "\n",
    "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
    "\n",
    "# 2. construct two models, one with random parameters, one with pre-trained parameters\n",
    "projection_dim = 256\n",
    "SCL = SCL(True, encoder, projection_dim, n_features)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SCL = SCL.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b56f49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.06845 -76.24047\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.RH.1981-2020.daymean.nc ...\n",
      "20.074585 -34.17395\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.T.1981-2020.daymean.nc ...\n",
      "26.455658 -35.592907\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.U.1981-2020.daymean.nc ...\n",
      "44.648796 -33.113792\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.V.1981-2020.daymean.nc ...\n",
      "7.01952 -5.8575945\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.W.1981-2020.daymean.nc ...\n",
      "3010.4414 -4328.3145\n",
      "loading data from /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/xdchen/anomaly/ERA5.SCL.850mb_anomaly.Z.1981-2020.daymean.nc ...\n",
      "(14610, 41, 81)\n"
     ]
    }
   ],
   "source": [
    "# 3. load data\n",
    "\n",
    "H_full = collect_norm_data_by_var('RH')\n",
    "T_full = collect_norm_data_by_var('T')\n",
    "U_full = collect_norm_data_by_var('U')\n",
    "V_full = collect_norm_data_by_var('V')\n",
    "W_full = collect_norm_data_by_var('W')\n",
    "Z_full = collect_norm_data_by_var('Z')\n",
    "\n",
    "print(Z_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2acb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(root_dir=datadir)\n",
    "\n",
    "# turn off shuffle, so data is processed in the time order\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f69af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93a6fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NT_XentLoss(torch.nn.Module):\n",
    "    def __init__(self, batch_size, temperature, device):\n",
    "        super(NT_XentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.mask = self._get_mask(batch_size)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_mask(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        N = 2 * self.batch_size\n",
    "        zis = F.normalize(zis, dim=1)\n",
    "        zjs = F.normalize(zjs, dim=1)\n",
    "        representations = torch.cat([zis, zjs], dim=0)\n",
    "\n",
    "        similarity_matrix = torch.matmul(representations, representations.T)\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "\n",
    "        positives = torch.cat([torch.diag(similarity_matrix, self.batch_size),\n",
    "                               torch.diag(similarity_matrix, -self.batch_size)], dim=0)\n",
    "        negatives = similarity_matrix[self.mask].view(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(self.device).long()\n",
    "        logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = loss / N\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1da0b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-02 10:25:06.484296\n",
      "step: 1\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [1/10], Loss: 5.5661\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "0 128\n",
      "step: 2\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [2/10], Loss: 5.5783\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "128 256\n",
      "step: 3\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [3/10], Loss: 5.5362\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "256 384\n",
      "step: 4\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [4/10], Loss: 5.6315\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "384 512\n",
      "step: 5\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [5/10], Loss: 5.5928\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "512 640\n",
      "step: 6\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [6/10], Loss: 5.5828\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "640 768\n",
      "step: 7\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [7/10], Loss: 5.5788\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "768 896\n",
      "step: 8\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [8/10], Loss: 5.5422\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "896 1024\n",
      "step: 9\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [9/10], Loss: 5.5791\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "1024 1152\n",
      "step: 10\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [10/10], Loss: 5.5740\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "1152 1280\n",
      "step: 11\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [11/10], Loss: 5.5462\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "1280 1408\n",
      "step: 12\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [12/10], Loss: 5.5334\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "1408 1536\n",
      "step: 13\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 512])\n",
      "Step [13/10], Loss: 5.5673\n",
      "2\n",
      "torch.Size([128, 512])\n",
      "1536 1664\n"
     ]
    }
   ],
   "source": [
    "outdata = np.zeros((2, H_full.shape[0], n_features))\n",
    "outindex = np.zeros(H_full.shape[0])\n",
    "\n",
    "sindex = -1*batch_size\n",
    "eindex = 0\n",
    "\n",
    "criterion = NT_XentLoss(batch_size=batch_size, temperature=0.5, device='cpu')\n",
    "optimizer = optim.Adam(SCL.parameters(), lr=0.001)\n",
    "\n",
    "print(dt.datetime.now())\n",
    "for step, data in enumerate(train_dataloader):\n",
    "    print(f'step: {step+1}')\n",
    "    optimizer.zero_grad()\n",
    "    testout = SCL(data[0].to(device), data[1].to(device))\n",
    "    print(testout[0].shape)\n",
    "    print(testout[1].shape)\n",
    "    loss = criterion(testout[0], testout[1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step [{step+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(len(testout))\n",
    "    print(testout[0].shape)\n",
    "\n",
    "\n",
    "    sindex = eindex\n",
    "    eindex += testout[0].shape[0]\n",
    "\n",
    "    outdata[0,sindex:eindex, :] = testout[0].detach().cpu().numpy()\n",
    "    outindex[sindex:eindex] = data[2]\n",
    "\n",
    "    print(sindex, eindex)\n",
    "\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7cc2155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to /global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/ResNet_output/RH_input/R18_output.anomaly.daymean.1981-2020.ERA5.mat ...\n"
     ]
    }
   ],
   "source": [
    "outfile = rootdir + 'ResNet_output/RH_input/%s_output.anomaly.daymean.1981-2020.ERA5.mat' % opt_model\n",
    "print('writing to %s ...' % outfile)\n",
    "description = 'Just the simCLR encoder output. So in 512 dimension. Use %s model' % opt_model\n",
    "script = '/global/cfs/projectdirs/m1657/liuy351/TallTower/From_XD/step02.ResNet_encoder_production_run.ipynb'\n",
    "sio.savemat(outfile, {'ResNetoutput':outdata, 'tindex':outindex, 'description':description, 'script':script})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
