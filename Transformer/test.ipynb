{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 17:00:29,312 - DEBUG - t500: -6.6453399658203125, 6.6453399658203125\n",
      "2024-06-28 17:00:29,328 - DEBUG - t850: -14.48138427734375, 14.48138427734375\n",
      "2024-06-28 17:00:29,350 - DEBUG - z500: -1520.5, 1520.5\n",
      "2024-06-28 17:00:29,371 - DEBUG - z850: -927.9365234375, 927.9365234375\n",
      "2024-06-28 17:00:29,388 - DEBUG - 2t: -17.207305908203125, 17.207305908203125\n",
      "2024-06-28 17:00:29,409 - DEBUG - sp: -952.28125, 952.28125\n",
      "2024-06-28 17:00:29,412 - DEBUG - Data shape after stacking and normalization: (109, 6, 37, 45)\n",
      "2024-06-28 17:00:29,416 - INFO - Starting feature extraction for 1 batches\n",
      "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 109, 6, 37, 45])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "2024-06-28 17:00:29,620 - INFO - Hidden states saved to hidden_states.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vit_pytorch import ViT\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import xarray as xr\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "datadir = '../ERA5_reduced/'\n",
    "\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, image_size=(40, 45), patch_size=5, dim=512, depth=6, heads=8, mlp_dim=2048, channels=6):\n",
    "        super(ViTFeatureExtractor, self).__init__()\n",
    "        self.vit = ViT(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=dim,  # Output dimension of the ViT\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            channels=channels\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "class ViT_BiLSTM_FeatureExtractor(nn.Module):\n",
    "    def __init__(self, image_size=(40, 45), patch_size=5, hidden_dim=512, num_layers=2):\n",
    "        super(ViT_BiLSTM_FeatureExtractor, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(image_size=image_size, patch_size=patch_size)\n",
    "        \n",
    "        # Bidirectional LSTM to model temporal dependencies\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=512,  # This should match the output dimension of ViT\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, num_channels, height, width = x.size()\n",
    "        \n",
    "        # Reshape to (batch_size * seq_length, num_channels, height, width)\n",
    "        x = x.view(batch_size * seq_length, num_channels, height, width)\n",
    "        \n",
    "        # Feature extraction using ViT\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Reshape to (batch_size, seq_length, feature_dim)\n",
    "        features = features.view(batch_size, seq_length, -1)\n",
    "        \n",
    "        # BiLSTM for temporal dependencies\n",
    "        bilstm_out, _ = self.bilstm(features)\n",
    "        \n",
    "        return bilstm_out\n",
    "\n",
    "def load_and_norm_single_variable_netcdf_data(my_var, vname, time=None):\n",
    "    fname = f'{datadir}/{my_var}.2001-2020.anomaly.nc'\n",
    "    \n",
    "    with xr.open_dataset(fname) as ds:\n",
    "        da = ds[vname]\n",
    "        if time is not None:\n",
    "            da = da.sel(time=slice(*time))\n",
    "        da_max = da.max(['time','latitude','longitude']).data\n",
    "        da_min = da.max(['time','latitude','longitude']).data\n",
    "        \n",
    "        my_vmax = np.maximum(da_max, -1*da_min)\n",
    "        vmax, vmin = my_vmax, -1*my_vmax\n",
    "        logging.debug(f'{my_var}: {vmin}, {vmax}')\n",
    "   \n",
    "        out = (da - vmin) / (vmax - vmin)\n",
    "        \n",
    "    return out\n",
    "\n",
    "def load_and_norm_netcdf_data(time=None):\n",
    "    t500_full = load_and_norm_single_variable_netcdf_data('t500', 'T', time=time)\n",
    "    t850_full = load_and_norm_single_variable_netcdf_data('t850', 'T', time=time)\n",
    "    z500_full = load_and_norm_single_variable_netcdf_data('z500', 'Z', time=time)\n",
    "    z850_full = load_and_norm_single_variable_netcdf_data('z850', 'Z', time=time)\n",
    "    t2_full   = load_and_norm_single_variable_netcdf_data('2t', 'VAR_2T', time=time)\n",
    "    sp_full   = load_and_norm_single_variable_netcdf_data('sp', 'SP', time=time)\n",
    "\n",
    "    data_list = [t500_full, t850_full, z500_full, z850_full, t2_full, sp_full]\n",
    "\n",
    "    time = data_list[0].time\n",
    "    for v in data_list:\n",
    "        time = np.intersect1d(time, v.time)\n",
    "\n",
    "    for v in data_list:\n",
    "        v = v.sel(time=time)\n",
    "    \n",
    "    normalized_data = np.stack(data_list, axis=1)\n",
    "\n",
    "    logging.debug(f\"Data shape after stacking and normalization: {normalized_data.shape}\")\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "# Ensure the input image size is compatible with the patch size\n",
    "def pad_image_to_fit_patch_size(data, patch_size):\n",
    "    _, _, height, width = data.shape\n",
    "    pad_height = (patch_size - height % patch_size) % patch_size\n",
    "    pad_width = (patch_size - width % patch_size) % patch_size\n",
    "    padded_data = np.pad(data, ((0, 0), (0, 0), (0, pad_height), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    return padded_data\n",
    "\n",
    "# Example usage to extract and store hidden states\n",
    "model = ViT_BiLSTM_FeatureExtractor(image_size=(37, 45), patch_size=(37, 45))\n",
    "\n",
    "# Load NetCDF data\n",
    "netcdf_file_path = 'path_to_your_netcdf_file.nc'\n",
    "input_data = load_and_norm_netcdf_data(time=('2016-06-26 00:00', '2016-06-30 12:00'))\n",
    "\n",
    "# # Pad image size to be compatible with patch size (e.g., 5x5)\n",
    "# input_data = pad_image_to_fit_patch_size(input_data, patch_size=5)\n",
    "\n",
    "# Convert input_data to torch tensor and ensure the correct shape\n",
    "input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "# Ensure input_data has shape (batch_size, num_channels, seq_length, height, width)\n",
    "# For example, if you have a single sequence of length 3 (hours), batch size 1:\n",
    "input_data = input_data.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "# Check if multiple GPUs are available and use DataParallel if so\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info(f\"Using {torch.cuda.device_count()} GPUs for model parallelism.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "input_data = input_data.to(device)\n",
    "\n",
    "# Process data in batches using tqdm for progress tracking\n",
    "batch_size = 1\n",
    "num_batches = input_data.size(0) // batch_size\n",
    "\n",
    "logging.info(f\"Starting feature extraction for {num_batches} batches\")\n",
    "all_hidden_states = []\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "    batch_data = input_data[i*batch_size:(i+1)*batch_size]\n",
    "    print(batch_data.shape)\n",
    "    hidden_states = model(batch_data)\n",
    "    all_hidden_states.append(hidden_states)\n",
    "\n",
    "# Concatenate all hidden states\n",
    "all_hidden_states = torch.cat(all_hidden_states, dim=0)\n",
    "\n",
    "# Save hidden states for later use\n",
    "torch.save(all_hidden_states, 'hidden_states.ViT.pt')\n",
    "logging.info(\"Hidden states saved to hidden_states.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 109, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0116,  0.0036,  0.0403,  ..., -0.0088, -0.0434, -0.0825],\n",
       "         [-0.0189,  0.0049,  0.0592,  ..., -0.0127, -0.0551, -0.0879],\n",
       "         [-0.0248,  0.0059,  0.0673,  ..., -0.0170, -0.0624, -0.0904],\n",
       "         ...,\n",
       "         [-0.0505,  0.0172,  0.0843,  ..., -0.0145, -0.0649, -0.0602],\n",
       "         [-0.0456,  0.0146,  0.0796,  ..., -0.0109, -0.0533, -0.0457],\n",
       "         [-0.0393,  0.0129,  0.0686,  ..., -0.0055, -0.0329, -0.0240]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_hidden_states.shape)\n",
    "all_hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
