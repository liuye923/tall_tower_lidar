{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e87c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import scipy.io as sio\n",
    "import datetime as dt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import rescale as skrescale\n",
    "from scipy import signal as ssignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a94b06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vstring = int(sys.argv[1])\n",
    "rseed = int(330)\n",
    "#humidvar = sys.argv[3]\n",
    "import random\n",
    "random.seed(rseed)\n",
    "np.random.seed(rseed)\n",
    "torch.manual_seed(rseed)\n",
    "torch.cuda.manual_seed(rseed)\n",
    "torch.cuda.manual_seed_all(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0553de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_model = 'R18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7890a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e87556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCL(nn.Module):\n",
    "    \"\"\"\n",
    "    We opt for simplicity and adopt the commonly used ResNet (He et al., 2016) to obtain hi = f(x ̃i) = ResNet(x ̃i) where hi ∈ Rd is the output after the average pooling layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train, encoder, projection_dim, n_features):\n",
    "        super(SCL, self).__init__()\n",
    "        self.train = train\n",
    "        self.encoder = encoder\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # increse input channel to 6\n",
    "        layer = self.encoder.conv1\n",
    "        new_nc = 6\n",
    "        new_layer = nn.Conv2d(in_channels=new_nc,\n",
    "                              out_channels=layer.out_channels,\n",
    "                              kernel_size=layer.kernel_size,\n",
    "                              stride=layer.stride,\n",
    "                              padding=layer.padding,\n",
    "                              bias=layer.bias)\n",
    "        # Extending the weights by copying from the old 3 to the new 3 channels\n",
    "        new_layer.weight.data[:, 0:3, :, :] = layer.weight.clone()\n",
    "        new_layer.weight.data[:, 3:6, :, :] = layer.weight.clone()\n",
    "        new_layer.weight = nn.Parameter(new_layer.weight)\n",
    "        self.encoder.conv1 = new_layer\n",
    "\n",
    "        # Replace the fc layer with an Identity function\n",
    "        self.encoder.fc = Identity()\n",
    "        # We use a MLP with one hidden layer to obtain z_i = g(h_i) = W(2)σ(W(1)h_i) where σ is a ReLU non-linearity.\n",
    "        # xc: This is the part that needs to be trained\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.n_features, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_features, projection_dim, bias=False),\n",
    "        )\n",
    "        # These are the parameters obtained from simCLR repo. I have also patched it to include 6 channels at the conv1\n",
    "        param_file = rootdir + 'model_lib/SCL_param.encoder.%s.6_channel.init.tar' % opt_model\n",
    "        self.encoder.load_state_dict(torch.load(param_file, map_location='cpu'))\n",
    "\n",
    "\n",
    "        # freeze the encoder so it is not re-trained\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x_i):\n",
    "        # z_i = self.encoder(x_i.type(torch.FloatTensor).cuda()).type(torch.FloatTensor).cuda()\n",
    "        z_i = self.encoder(x_i.type(torch.FloatTensor)).type(torch.FloatTensor)\n",
    "\n",
    "        del x_i\n",
    "        return z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58edccb-abec-49c2-a559-1e54ac75bb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_norm_data_by_var(my_var, vname):\n",
    "    \n",
    "    fname = f'{datadir}/{my_var}.2001-2020.daymean.anomaly.nc'\n",
    "    with xr.open_dataset(fname) as ds:\n",
    "        da = ds[vname]\n",
    "        da_max = da.max(['time','latitude','longitude']).data\n",
    "        da_min = da.max(['time','latitude','longitude']).data\n",
    "        \n",
    "        my_vmax = np.maximum(da_max, -1*da_min)\n",
    "        vmax, vmin = my_vmax, -1*my_vmax\n",
    "        print(f'{my_var}: {vmin}, {vmax}')\n",
    "        \n",
    "        out = (da - vmin) / (vmax - vmin)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695ffd19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean5kernel = np.ones((5,5))/25\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    '''\n",
    "    Since we need to mannually normalize the data, let's create datasets elsewhere, and just aggreagate them here.\n",
    "    Requires: T_full, H_full, W_full, U_full, V_full, Z_full\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_data = root_dir + 't500.2001-2020.daymean.anomaly.nc'\n",
    "\n",
    "    def __len__(self):\n",
    "        with xr.open_dataset(self.sample_data) as inds:\n",
    "            nt = inds['T'].shape[0]\n",
    "        return int(nt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # find a corresponding idx_pair, outside the 360-length window of idx\n",
    "        # idx_pair = xxxx\n",
    "\n",
    "\n",
    "        sample_raw = np.zeros((6,92,112))\n",
    "\n",
    "        for i,fullds in zip(np.arange(6), [t500_full, t850_full, z500_full, z850_full, t2_full, sp_full]):\n",
    "            \n",
    "            # construct input for idx\n",
    "            # rescaling\n",
    "            data_step1 = skrescale(fullds[idx], (2.5, 2.5), anti_aliasing=True)\n",
    "            # mean using 5x5\n",
    "            data_step2 = ssignal.convolve2d(data_step1, mean5kernel, boundary='symm', mode='same')\n",
    "            sample_raw[i] = data_step2\n",
    "\n",
    "\n",
    "            # construct input for idx_pair\n",
    "            # rescaling\n",
    "\n",
    "            # mean using 5x5\n",
    "\n",
    "        return sample_raw, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a9ae28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SCLloss(my_x, my_y, my_temperature=0.5):\n",
    "    '''\n",
    "    my_x and my_y has a one-to-one pair. So there are in total N*N pairs. In these N*N, the diagonal pairs are positive,\n",
    "     and the rest are negative. So we want to maximum diagonal while suppressing the rest.\n",
    "    '''\n",
    "    ns = my_x.shape[0]\n",
    "    # use broadcasting to achieve pairwise cos. Note my_y.t() operation and dimension handling\n",
    "    cos_matrix = torch.nn.functional.cosine_similarity(my_x[:,:,None], my_y.t()[None,:,:])/my_temperature\n",
    "    similarity_matrix = torch.exp(cos_matrix)\n",
    "\n",
    "\n",
    "    loss = torch.tensor([0.0], requires_grad=True)\n",
    "    for i in np.arange(ns):\n",
    "        loss = loss -1*torch.log(similarity_matrix[i,i]/(torch.sum(similarity_matrix[i,:])-similarity_matrix[i,i]))\n",
    "        loss = loss -1*torch.log(similarity_matrix[i,i]/(torch.sum(similarity_matrix[:,i])-similarity_matrix[i,i]))\n",
    "\n",
    "    loss = loss/(2*ns)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2f3fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootdir = '/global/cfs/projectdirs/m1657/liuy351/TallTower/SCL/'\n",
    "\n",
    "datadir = '/global/cfs/projectdirs/m1657/liuy351/TallTower/ERA5_reduced/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "357b07f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. major parameters\n",
    "if opt_model=='R18':\n",
    "    batch_size = 128\n",
    "elif opt_model=='R15':\n",
    "    batch_size = 64\n",
    "    \n",
    "# 1. construct functions\n",
    "if opt_model=='R18':\n",
    "    encoder = torchvision.models.resnet18(weights=None)\n",
    "elif opt_model=='R50':\n",
    "    encoder = torchvision.models.resnet50(weights=None)\n",
    "\n",
    "n_features = encoder.fc.in_features  # get dimensions of fc layer\n",
    "\n",
    "# 2. construct two models, one with random parameters, one with pre-trained parameters\n",
    "projection_dim = 256\n",
    "SCL = SCL(True, encoder, projection_dim, n_features)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SCL = SCL.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b56f49a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t500: -271.6439208984375, 271.6439208984375\n",
      "t850: -307.06951904296875, 307.06951904296875\n",
      "z500: -58777.48046875, 58777.48046875\n",
      "z850: -16449.490234375, 16449.490234375\n",
      "2t: -310.2423400878906, 310.2423400878906\n",
      "sp: -104705.5390625, 104705.5390625\n",
      "(7305, 37, 45)\n"
     ]
    }
   ],
   "source": [
    "# 3. load data\n",
    "\n",
    "t500_full = collect_norm_data_by_var('t500', 'T')\n",
    "t850_full = collect_norm_data_by_var('t850', 'T')\n",
    "z500_full = collect_norm_data_by_var('z500', 'Z')\n",
    "z850_full = collect_norm_data_by_var('z850', 'Z')\n",
    "t2_full   = collect_norm_data_by_var('2t', 'VAR_2T')\n",
    "sp_full   = collect_norm_data_by_var('sp', 'SP')\n",
    "\n",
    "print(t500_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449f8dd4-e3bb-4b07-a13d-1a6d4a08b0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2acb773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(root_dir=datadir)\n",
    "\n",
    "# turn off shuffle, so data is processed in the time order\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1da0b748",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 11:57:59.156274\n",
      "0 128\n",
      "128 256\n",
      "256 384\n",
      "384 512\n",
      "512 640\n",
      "640 768\n",
      "768 896\n",
      "896 1024\n",
      "1024 1152\n",
      "1152 1280\n",
      "1280 1408\n",
      "1408 1536\n",
      "1536 1664\n",
      "1664 1792\n",
      "1792 1920\n",
      "1920 2048\n",
      "2048 2176\n",
      "2176 2304\n",
      "2304 2432\n",
      "2432 2560\n",
      "2560 2688\n",
      "2688 2816\n",
      "2816 2944\n",
      "2944 3072\n",
      "3072 3200\n",
      "3200 3328\n",
      "3328 3456\n",
      "3456 3584\n",
      "3584 3712\n",
      "3712 3840\n",
      "3840 3968\n",
      "3968 4096\n",
      "4096 4224\n",
      "4224 4352\n",
      "4352 4480\n",
      "4480 4608\n",
      "4608 4736\n",
      "4736 4864\n",
      "4864 4992\n",
      "4992 5120\n",
      "5120 5248\n",
      "5248 5376\n",
      "5376 5504\n",
      "5504 5632\n",
      "5632 5760\n",
      "5760 5888\n",
      "5888 6016\n",
      "6016 6144\n",
      "6144 6272\n",
      "6272 6400\n",
      "6400 6528\n",
      "6528 6656\n",
      "6656 6784\n",
      "6784 6912\n",
      "6912 7040\n",
      "7040 7168\n",
      "7168 7296\n",
      "7296 7305\n",
      "2024-06-25 12:00:59.490795\n"
     ]
    }
   ],
   "source": [
    "outdata = np.zeros((t500_full.shape[0], n_features))\n",
    "outindex = np.zeros(t500_full.shape[0])\n",
    "\n",
    "sindex = -1*batch_size\n",
    "eindex = 0\n",
    "\n",
    "print(dt.datetime.now())\n",
    "for step, data in enumerate(train_dataloader):\n",
    "    # print(data[0].shape)\n",
    "\n",
    "    testout = SCL(data[0].to(device))\n",
    "\n",
    "\n",
    "    sindex = eindex\n",
    "    eindex += testout.shape[0]\n",
    "\n",
    "    outdata[sindex:eindex, :] = testout.detach().cpu().numpy()\n",
    "    outindex[sindex:eindex] = data[1]\n",
    "\n",
    "    print(sindex, eindex)\n",
    "\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7cc2155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to /global/cfs/projectdirs/m1657/liuy351/TallTower/SCL/ResNet_output/R18_output.anomaly.2001-2020.ERA5.mat ...\n"
     ]
    }
   ],
   "source": [
    "outfile = rootdir + 'ResNet_output/%s_output.anomaly.2001-2020.ERA5.mat' % opt_model\n",
    "print('writing to %s ...' % outfile)\n",
    "description = 'Just the simCLR encoder output. So in 512 dimension. Use %s model' % opt_model\n",
    "script = '/global/cfs/projectdirs/m1657/liuy351/TallTower/SCL/02.ResNet_encoder_production_run.ipynb'\n",
    "sio.savemat(outfile, {'ResNetoutput':outdata, 'tindex':outindex, 'description':description, 'script':script})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34c3a725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da6491-bea2-4efe-bd70-8c59e1126278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
